# ğŸ“„ Resume RAG Engine

> **Smart resume search powered by AI** - Upload PDFs, ask questions, get intelligent answers with source citations.

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.110+-green.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://www.docker.com/)

## ğŸ¯ Overview

A production-ready Resume Discovery API that uses semantic search and LLM-powered question answering to find and analyze resumes. Built with privacy-first design including PII redaction and anonymous IDs.

```mermaid
flowchart LR
    subgraph Input
        PDF[ğŸ“„ PDF Resume]
        Q[â“ Question]
    end
    
    subgraph Processing
        Extract[ğŸ¤– Gemini<br/>Extract Fields]
        Chunk[âœ‚ï¸ Semantic<br/>Chunker]
        Embed[ğŸ”¢ HuggingFace<br/>Embeddings]
        Search[ğŸ” Vector<br/>Search]
        Rerank[ğŸ“Š Reranker]
        Answer[ğŸ’¬ Gemini<br/>Answer]
    end
    
    subgraph Storage
        DB[(ChromaDB)]
        Redis[(Redis Queue)]
    end
    
    PDF --> Extract --> Chunk --> Embed --> DB
    Q --> Search --> DB
    DB --> Rerank --> Answer
    PDF -.-> Redis -.-> Extract
```

## âœ¨ Features

- **ğŸ” Semantic Search** - Find resumes by meaning, not just keywords
- **ğŸ¤– AI-Powered Q&A** - Ask natural language questions about candidates
- **ğŸ“„ PDF Processing** - Extract text and metadata from resumes
- **ğŸ”’ Privacy-First** - PII redaction and anonymous candidate IDs
- **âš¡ Async Processing** - Optional Redis queue for bulk uploads
- **ğŸ¯ Multi-hop Retrieval** - Complex query decomposition for better answers
- **ğŸ“Š Confidence Scores** - Know how reliable each answer is

## ğŸš€ Quick Start

### Option 1: Docker (Recommended)

```bash
# 1. Clone the repository
git clone https://github.com/your-repo/mcp-resume-engine.git
cd mcp-resume-engine

# 2. Create .env file
echo "GEMINI_API_KEY=your_gemini_key" > .env
echo "HUGGINGFACE_API_TOKEN=your_hf_token" >> .env

# 3. Start with Docker Compose
docker-compose up --build

# 4. Open API docs
# http://localhost:8000/docs
```

### Option 2: Local Development

```bash
# 1. Create virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac

# 2. Install dependencies
pip install -r requirements.txt

# 3. Set environment variables
export GEMINI_API_KEY=your_key
export HUGGINGFACE_API_TOKEN=your_token

# 4. Run the server
uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000
```

## ğŸ“ Project Structure

```
mcp-resume-engine/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ core/              # Text processing & semantic chunking
â”‚   â”‚   â”œâ”€â”€ chunker.py     # Section-aware resume chunking
â”‚   â”‚   â””â”€â”€ preprocessor.py # Text cleaning & normalization
â”‚   â”œâ”€â”€ embeddings/        # Vector embeddings & storage
â”‚   â”‚   â”œâ”€â”€ embedder.py    # HuggingFace API embeddings
â”‚   â”‚   â””â”€â”€ vectorstore.py # ChromaDB vector store
â”‚   â”œâ”€â”€ retrieval/         # Search & reranking
â”‚   â”‚   â”œâ”€â”€ reranker.py    # Result reranking with skill matching
â”‚   â”‚   â”œâ”€â”€ multihop.py    # Multi-hop retrieval for complex queries
â”‚   â”‚   â””â”€â”€ verifier.py    # Confidence scoring
â”‚   â”œâ”€â”€ queue/             # Redis queue infrastructure
â”‚   â”‚   â”œâ”€â”€ worker.py      # Background job processor
â”‚   â”‚   â””â”€â”€ jobs.py        # Job definitions
â”‚   â”œâ”€â”€ main.py            # FastAPI endpoints
â”‚   â”œâ”€â”€ llm.py             # Gemini LLM integration
â”‚   â”œâ”€â”€ models.py          # Pydantic data models
â”‚   â””â”€â”€ anonymizer.py      # PII redaction
â”œâ”€â”€ docker-compose.yml     # Docker orchestration
â”œâ”€â”€ Dockerfile             # Container build
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md
```

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/ingest_pdf` | Upload and process a resume PDF |
| `POST` | `/qa` | Ask questions about resumes |
| `GET` | `/health` | Health check with queue status |
| `GET` | `/resumes` | List all processed resumes |
| `DELETE` | `/resumes/{id}` | Delete a resume |

### Upload Resume

```bash
curl -X POST "http://localhost:8000/ingest_pdf" \
  -F "file=@resume.pdf"
```

**Response:**
```json
{
  "status": "processed",
  "id": "abc123",
  "name": "John Doe",
  "skills": ["Python", "React", "AWS"],
  "chunk_count": 8
}
```

### Ask Question

```bash
curl -X POST "http://localhost:8000/qa" \
  -H "Content-Type: application/json" \
  -d '{"question": "Who has Python and machine learning experience?", "top_k": 5}'
```

**Response:**
```json
{
  "answer": "John Doe has extensive Python and ML experience...",
  "confidence_score": 0.92,
  "sources": [...],
  "is_fallback": false
}
```

### Async Upload (Bulk Processing)

```bash
curl -X POST "http://localhost:8000/ingest_pdf?async_mode=true" \
  -F "file=@resume.pdf"
```

Returns `job_id` for status checking.

## âš™ï¸ Architecture

### Ingestion Pipeline

```mermaid
sequenceDiagram
    participant User
    participant API
    participant LLM as Gemini LLM
    participant HF as HuggingFace API
    participant Vec as ChromaDB
    
    User->>API: Upload PDF
    API->>API: Extract text & strip PII
    API->>LLM: Extract fields (name, skills, projects)
    API->>API: Semantic chunking by sections
    API->>HF: Generate embeddings
    HF->>API: 384-dim vectors
    API->>Vec: Store chunks + metadata
    API->>User: âœ“ Resume processed
```

### Question Answering Pipeline

```mermaid
sequenceDiagram
    participant User
    participant API
    participant HF as HuggingFace API
    participant Vec as ChromaDB
    participant LLM as Gemini LLM
    
    User->>API: Ask question
    API->>API: Expand query
    API->>HF: Embed question
    HF->>API: Query vector
    API->>Vec: Semantic search (top 15)
    Vec->>API: Candidate chunks
    API->>API: Rerank by skills & sections
    API->>LLM: Generate answer with context
    LLM->>API: Natural language answer
    API->>User: Answer + confidence + sources
```

## ğŸ³ Docker Compose Services

```yaml
services:
  api:        # FastAPI server (port 8000)
  redis:      # Job queue (port 6379)
  worker:     # Background processors (5 replicas)
```

**Scaling workers:**
```bash
docker-compose up --scale worker=10
```

## ğŸ”§ Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `GEMINI_API_KEY` | âœ… | Google Gemini API key for LLM |
| `HUGGINGFACE_API_TOKEN` | âœ… | HuggingFace API token for embeddings |
| `ALLOWED_ORIGINS` | âŒ | CORS origins (default: `*`) |
| `REDIS_URL` | âŒ | Redis connection URL (auto-configured in Docker) |

## ğŸ“Š Tech Stack

| Component | Technology |
|-----------|------------|
| **API Framework** | FastAPI + Pydantic |
| **LLM** | Google Gemini 2.5 Flash |
| **Embeddings** | HuggingFace Inference API (MiniLM-L6-v2) |
| **Vector Store** | ChromaDB |
| **Queue** | Redis + RQ |
| **Container** | Docker + Docker Compose |

## ğŸ”’ Privacy Features

- **PII Redaction**: Phone numbers, emails, addresses automatically masked
- **Anonymous IDs**: Hash-based candidate identifiers
- **No Raw Storage**: Original PDFs are not stored, only processed text

## ğŸ“ License

MIT
